import json
import os
import re
import argparse
import math

BENCHMARK_RESULTS_FILE = "output/benchmark_results.json"
OUTPUT_TEX_FILE = "output/solutions_report.tex"

def escape_latex(text):
    """
    Escapes special LaTeX characters in a given string.
    """
    if not isinstance(text, str):
        return ""
    # More robust escaping
    text = text.replace('\\', r'\textbackslash{}')
    text = text.replace('&', r'\&')
    text = text.replace('%', r'\%')
    text = text.replace('$', r'\$')
    text = text.replace('#', r'\#')
    text = text.replace('_', r'\_')
    text = text.replace('{', r'\{')
    text = text.replace('}', r'\}')
    text = text.replace('~', r'\textasciitilde{}')
    text = text.replace('^', r'\textasciicircum{}')
    # Handle unicode characters found in the log
    text = text.replace(' ', ' ')
    text = text.replace('⟨', r'$\langle$')
    text = text.replace('⟩', r'$\rangle$')
    text = text.replace('θ', r'$\theta$')
    text = text.replace('α', r'$\alpha$')
    text = text.replace('β', r'$\beta$')
    text = text.replace('√', r'$\sqrt{}$')

    return text

TEX_TEMPLATE_HEADER = r"""
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{array}
\usepackage{lmodern}
\usepackage{fancyvrb}

\geometry{a4paper, margin=1in}

\definecolor{correct}{HTML}{28a745}
\definecolor{partial}{HTML}{FFC107}
\definecolor{incorrect}{HTML}{DC3545}
\definecolor{noerror}{HTML}{6C757D}

\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\dvol}{\mathrm{d}V}

\title{LLM Benchmark Report}
\author{Generated by script}
\date{\today}

\begin{document}
\maketitle
"""

TEX_TEMPLATE_FOOTER = r"""
\end{document}
"""

def generate_summary_table(summary_data):
    """Generates the LaTeX for the summary statistics table."""
    if not summary_data:
        return ""

    table_header = r"""
\section*{Summary Statistics}
\begin{center}
\begin{longtable}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Total} & \textbf{Correct} & \textbf{Partial} & \textbf{Incorrect} & \textbf{Errors} & \textbf{Avg. Score} \\
\hline
\endfirsthead
\hline
\endfoot
"""
    table_rows = ""
    for model, stats in summary_data.items():
        table_rows += f"{escape_latex(model)} & {stats.get('total', 0)} & {stats.get('1_count', 0)} & {stats.get('0.5_count', 0)} & {stats.get('0_count', 0)} & {stats.get('null_count', 0)} & {stats.get('average_score', 0.0):.2f} \\\\\n"

    table_footer = r"""\hline
\end{longtable}
\end{center}
"""
    return table_header + table_rows + table_footer


def generate_problem_parts(results_data, base_tex_dir, num_files=7):
    """Generates LaTeX for detailed problems, split into multiple files."""
    if not results_data:
        return []

    problems_dir_name = "problem_parts"
    problems_dir = os.path.join(base_tex_dir, problems_dir_name)
    os.makedirs(problems_dir, exist_ok=True)

    problems_per_file = math.ceil(len(results_data) / num_files)
    problem_files = []

    for i in range(num_files):
        start_index = i * problems_per_file
        end_index = start_index + problems_per_file
        chunk = results_data[start_index:end_index]

        if not chunk:
            continue

        part_num = i + 1
        part_filename = f"problems_part_{part_num}.tex"
        part_filepath = os.path.join(problems_dir, part_filename)
        # Use forward slashes for LaTeX \input command
        relative_path = os.path.join(problems_dir_name, part_filename).replace(os.path.sep, '/')
        problem_files.append(relative_path)

        with open(part_filepath, 'w', encoding='utf-8') as f:
            f.write(f"\\section*{{Problem Details - Part {part_num}}}\n\n")
            
            for problem_idx, problem in enumerate(chunk):
                global_problem_idx = start_index + problem_idx
                paper_id = escape_latex(problem.get("paper_id", "N/A"))
                problem_statement = problem.get("problem_statement", "")
                ground_truth_solution = problem.get("ground_truth_solution", "")

                f.write(f"\\subsection*{{Problem {global_problem_idx+1} (Paper: {paper_id})}}\n")
                f.write("\\subsubsection*{Problem Statement}\n")
                f.write(f"{problem_statement}\n\n")
                
                f.write("\\subsubsection*{Ground Truth Solution}\n")
                if ground_truth_solution.strip():
                    solution = ground_truth_solution.strip()
                    if "\\boxed{" in solution:
                        f.write(f"\\[ {solution} \\]\n\n")
                    else:
                        f.write(f"\\[ \\boxed{{{solution}}} \\]\n\n")
                else:
                    f.write("No ground truth solution provided.\n\n")
                
                f.write("\\subsubsection*{Model Outputs}\n")
                
                for model_output_name, output in problem.get("model_outputs", {}).items():
                    score = output.get('score')
                    score_str = str(score) if score is not None else "N/A"
                    
                    f.write(f"\\subsubsection*{{Model: {escape_latex(model_output_name)} (Score: {score_str})}}\n")
                    
                    model_solution = output.get("solution", "No solution provided.")
                    f.write("\\paragraph*{Model Solution:}\n")
                    if model_solution.strip() and not model_solution.strip().startswith("No solution"):
                        f.write(f"\\[ {model_solution} \\]\n\n")
                    else:
                        f.write(f"{escape_latex(model_solution)}\n\n")
                    
                    evaluation = output.get("evaluation", "No evaluation provided.")
                    f.write("\\paragraph*{Judge's Evaluation:}\n")
                    f.write(f"\n{escape_latex(evaluation)}\n\n")
                    
                f.write("\\newpage\n")
    
    return problem_files


def export_benchmark_to_tex(benchmark_results_file, output_tex_file, model_name, correct_only=False):
    """
    Reads benchmark results and generates a LaTeX report.
    """
    if not os.path.exists(benchmark_results_file):
        print(f"Error: Benchmark results file not found at '{benchmark_results_file}'")
        return

    with open(benchmark_results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
 
    # Optionally filter to only include problems the specified model answered correctly
    if correct_only:
        original_count = len(data.get("results", []))
        filtered_results = []
        for prob in data.get("results", []):
            model_out = prob.get("model_outputs", {}).get(model_name, {})
            if model_out.get("score") == 1.0:
                filtered_results.append(prob)

        data["results"] = filtered_results
        print(f"Filtered problems: {len(filtered_results)} of {original_count} (only correct ones).")

    base_tex_dir = os.path.dirname(output_tex_file)

    pdf_dir = os.path.join(os.path.dirname(base_tex_dir), "pdf")
    os.makedirs(pdf_dir, exist_ok=True)

    # Generate content
    summary_table_tex = generate_summary_table(data.get("summary", {}))
    problem_files = generate_problem_parts(data.get("results", []), base_tex_dir, num_files=10)

    # Combine all parts
    final_tex = TEX_TEMPLATE_HEADER
    final_tex += summary_table_tex
    
    for problem_file in problem_files:
        final_tex += f"\\input{{{problem_file}}}\n"

    final_tex += TEX_TEMPLATE_FOOTER

    # Write to file
    with open(output_tex_file, 'w', encoding='utf-8') as f:
        f.write(final_tex)

    print(f"LaTeX report successfully generated at '{output_tex_file}'")
    problem_parts_dir = os.path.join(base_tex_dir, 'problem_parts')
    print("The report is split into multiple parts in:")
    print(f"  - {problem_parts_dir}")
    
    main_tex_file_rel_path = os.path.relpath(output_tex_file, os.getcwd())
    pdf_dir_rel_path = os.path.relpath(pdf_dir, os.getcwd())
    print("\nYou can now compile the main file using a LaTeX distribution to create a PDF:")
    print(f"pdflatex -output-directory={pdf_dir_rel_path} {main_tex_file_rel_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Export benchmark results to LaTeX")
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="The model name for which to generate the report (e.g., o3-mini, gpt-4o)."
    )
    parser.add_argument(
        "--correct-only",
        action="store_true",
        help="Include only problems that the specified model answered correctly."
    )
    args = parser.parse_args()

    original_model_name = args.model  # Keep the exact name used in benchmark results

    # Sanitise model name for directory/file paths
    sanitized_model_name = original_model_name.replace("openai/", "").replace("/", "-")

    model_results_dir = f"output/results/{sanitized_model_name}"

    benchmark_results_file = f"{model_results_dir}/benchmark_results_{sanitized_model_name}.json"
    
    tex_dir = os.path.join(model_results_dir, "tex")
    os.makedirs(tex_dir, exist_ok=True)
    
    output_tex_file = os.path.join(tex_dir, f"solutions_report_{sanitized_model_name}.tex")

    export_benchmark_to_tex(
        benchmark_results_file,
        output_tex_file,
        original_model_name,
        correct_only=args.correct_only,
    ) 