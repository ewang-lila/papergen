#!/usr/bin/env python3
"""Extract problems that a given model answered perfectly.

Usage:

    # Basic usage - paths are inferred from the model name
    python export_correct_problems.py --model o3-mini

    # Explicitly specify paths
    python export_correct_problems.py \
        --model o3-mini \
        --benchmark path/to/your/benchmark_results_o3-mini.json \
        --output path/to/your/o3_correct_problems.json

The script reads the benchmark results file (format produced by *benchmark_llms.py*),
filters for problems where the chosen model has score == 1 (or 1.0), and
writes a new JSON file in the same schema as *refined_problems.json* so that it
can be fed directly into *generate_solution_traces.py*.
"""
from __future__ import annotations

import argparse
import json
import os
from collections import defaultdict
from typing import Any, Dict, List


def load_benchmark(path: str) -> List[Dict[str, Any]]:
    """Load benchmark JSON and return list of result entries."""
    with open(path, "r", encoding="utf-8") as fp:
        obj = json.load(fp)

    # Some benchmark files store the list under the key "results",
    # others may already be a list.
    if isinstance(obj, dict) and "results" in obj:
        return obj["results"]
    if isinstance(obj, list):
        return obj

    raise ValueError("Unrecognized benchmark JSON structure – expected list or dict with 'results'.")


def collect_correct_problems(results: List[Dict[str, Any]], model_key: str) -> List[Dict[str, Any]]:
    """Return problems where the given model scored perfectly (1.0) **or** half-credit (0.5).

    The benchmark currently assigns scores of 0, 0.5, or 1. A score ≥ 0.5 is
    deemed "acceptable" for the purposes of generating solution traces, so we
    include both 0.5 and 1.0 here.  The numeric comparison is tolerant to small
    floating-point error (~1e-6).
    """
    papers: Dict[str, List[Dict[str, str]]] = defaultdict(list)

    for entry in results:
        paper_id = entry.get("paper_id")
        if not paper_id:
            continue

        # Extract score (may be str or number)
        raw_score = (
            entry.get("model_outputs", {})
            .get(model_key, {})
            .get("score")
        )

        try:
            score_val = float(raw_score)
        except (TypeError, ValueError):
            continue  # Skip entries with missing or non-numeric scores

        # Accept scores >= 0.5 (tolerant to tiny FP error)
        if score_val < 0.5:
            continue

        papers[paper_id].append(
            {
                "problem_statement": entry.get("problem_statement", ""),
                "final_solution": entry.get("ground_truth_solution", ""),
            }
        )

    # Convert to list in required schema
    return [
        {"paper_id": pid, "problems": probs} for pid, probs in papers.items()
    ]


def write_output(correct: List[Dict[str, Any]], path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as fp:
        json.dump(correct, fp, indent=4, ensure_ascii=False)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Export problems a model solved perfectly to a new JSON file.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("--model", required=True, help="Model key to inspect (e.g., 'o3-mini').")
    parser.add_argument("--output-dir", type=str, default="output", help="The base directory for all output files.")
    parser.add_argument("--benchmark", help="Path to benchmark_results JSON file. If not provided, it will be inferred from the model name.")
    parser.add_argument("--output", help="Destination JSON file for extracted problems. If not provided, it will be inferred from the model name.")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite the output file if it exists, instead of appending.")

    args = parser.parse_args()
    
    # Infer paths if not provided
    if not args.benchmark:
        args.benchmark = os.path.join(args.output_dir, f"results/{args.model}/benchmark_results_{args.model}.json")
    
    if not args.output:
        args.output = os.path.join(args.output_dir, f"problems/{args.model}_correct_problems.json")

    try:
        results = load_benchmark(args.benchmark)
    except Exception as exc:
        print(f"[ERROR] Failed to load benchmark file: {exc}")
        return

    newly_correct = collect_correct_problems(results, args.model)

    existing_correct = []
    if not args.overwrite and os.path.exists(args.output):
        print(f"Output file {args.output} exists. Appending new results.")
        try:
            with open(args.output, "r", encoding="utf-8") as fp:
                existing_correct = json.load(fp)
            if not isinstance(existing_correct, list):
                print(f"[WARN] Existing file {args.output} has an unexpected format and will be overwritten.")
                existing_correct = []
        except (json.JSONDecodeError, FileNotFoundError):
            print(f"[WARN] Could not read or parse {args.output}. It will be overwritten.")
            existing_correct = []

    # Use a dictionary to merge and deduplicate problems
    all_problems_by_paper: Dict[str, Dict[str, Dict[str, str]]] = defaultdict(dict)

    # Process existing problems first
    for paper in existing_correct:
        paper_id = paper.get("paper_id")
        if paper_id:
            for problem in paper.get("problems", []):
                if "problem_statement" in problem:
                    all_problems_by_paper[paper_id][problem["problem_statement"]] = problem

    # Process newly found problems, overwriting duplicates to ensure data is fresh
    for paper in newly_correct:
        paper_id = paper.get("paper_id")
        if paper_id:
            for problem in paper.get("problems", []):
                if "problem_statement" in problem:
                    all_problems_by_paper[paper_id][problem["problem_statement"]] = problem

    # Reconstruct the list in the correct schema
    final_problems_list = [
        {"paper_id": pid, "problems": list(probs.values())}
        for pid, probs in all_problems_by_paper.items()
    ]

    if not final_problems_list:
        print("[WARN] No perfectly solved problems found to write.")
        return

    write_output(final_problems_list, args.output)

    total = sum(len(p["problems"]) for p in final_problems_list)
    print(f"Wrote {len(final_problems_list)} papers, {total} total problems to {args.output}.")


if __name__ == "__main__":
    main() 